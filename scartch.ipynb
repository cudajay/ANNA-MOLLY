{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from matplotlib import pyplot as plt\n",
    "from keras.callbacks import History, EarlyStopping, Callback\n",
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)\n",
    "import glob as glb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jmeow/git/ANNA-MOLLY\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/jmeow/git/ANNA-MOLLY/telemanom/telemanom'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('telemanom')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from channel import Channel\n",
    "from helpers import Config\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cfg = Config(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rando_data(n = 5):\n",
    "    ret = []\n",
    "    g = glb.glob(\"../data/train/*\")\n",
    "    sel = random.choice(g).split(\"/\")[-1].split(\".\")[0]\n",
    "    os.chdir('../')\n",
    "    for _ in range(n):\n",
    "        ch = Channel(cfg, sel)\n",
    "        ch.load_data()\n",
    "        ret.append(ch)\n",
    "    os.chdir('telemanom')\n",
    "    return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = get_rando_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from model_utils import TimeGAN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ch[1]\n",
    "nfeatures = c.X_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Channel' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [31]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Channel' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "c.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jmeow/git/ANNA-MOLLY/telemanom/telemanom\r\n"
     ]
    }
   ],
   "source": [
    "from"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(2620, 250, 25)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.X_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Input, Concatenate, Reshape, GRU\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten, RepeatVector\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Input(shape=(250, 25), name='RealData')\n",
    "Z = Input(shape=(250, 25), name='RandomData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seq_len = 1\n",
    "n_seq = 25\n",
    "batch_size = 250\n",
    "hidden_dim = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def make_rnn(n_layers, hidden_units, output_units, name):\n",
    "    return Sequential([GRU(units=hidden_units,\n",
    "                           return_sequences=True,\n",
    "                           name=f'GRU_{i + 1}') for i in range(n_layers)] +\n",
    "                      [Dense(units=output_units,\n",
    "                             activation='sigmoid',\n",
    "                             name='OUT')], name=name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "embedder = make_rnn(n_layers=3,\n",
    "                    hidden_units=hidden_dim,\n",
    "                    output_units=hidden_dim,\n",
    "                    name='Embedder')\n",
    "recovery = make_rnn(n_layers=3,\n",
    "                    hidden_units=hidden_dim,\n",
    "                    output_units=n_seq,\n",
    "                    name='Recovery')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "generator = make_rnn(n_layers=3,\n",
    "                     hidden_units=hidden_dim,\n",
    "                     output_units=hidden_dim,\n",
    "                     name='Generator')\n",
    "discriminator = make_rnn(n_layers=3,\n",
    "                         hidden_units=hidden_dim,\n",
    "                         output_units=1,\n",
    "                         name='Discriminator')\n",
    "supervisor = make_rnn(n_layers=2,\n",
    "                      hidden_units=hidden_dim,\n",
    "                      output_units=hidden_dim,\n",
    "                      name='Supervisor')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "train_steps = 10000\n",
    "gamma = 1\n",
    "mse = MeanSquaredError()\n",
    "bce = BinaryCrossentropy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "H = embedder(X)\n",
    "X_tilde = recovery(H)\n",
    "\n",
    "autoencoder = Model(inputs=X,\n",
    "                    outputs=X_tilde,\n",
    "                    name='Autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " RealData (InputLayer)       [(None, 250, 25)]         0         \n",
      "                                                                 \n",
      " Embedder (Sequential)       (None, 250, 10)           2540      \n",
      "                                                                 \n",
      " Recovery (Sequential)       (None, 250, 25)           2255      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,795\n",
      "Trainable params: 4,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_autoencoder_init(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_tilde = autoencoder(x)\n",
    "        embedding_loss_t0 = mse(x, x_tilde)\n",
    "        e_loss_0 = 10 * tf.sqrt(embedding_loss_t0)\n",
    "\n",
    "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "    gradients = tape.gradient(e_loss_0, var_list)\n",
    "    autoencoder_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return tf.sqrt(embedding_loss_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "train_steps = c.X_train.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 100/2620 [01:41<43:03,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.25249106>, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 200/2620 [03:19<44:54,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.25461343>, 199)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 300/2620 [04:57<43:24,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.2398998>, 299)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 400/2620 [06:37<40:23,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.24383248>, 399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 500/2620 [08:18<35:58,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.2415181>, 499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 600/2620 [10:00<33:03,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.2290865>, 599)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 700/2620 [11:44<32:01,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.23603395>, 699)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 800/2620 [13:27<29:32,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.23411117>, 799)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 900/2620 [15:08<28:27,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.23939343>, 899)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1000/2620 [16:48<27:53,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.25436586>, 999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 1100/2620 [18:29<23:34,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.24689044>, 1099)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 1200/2620 [20:10<22:54,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.23107731>, 1199)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 1300/2620 [21:49<21:47,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.24244604>, 1299)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1400/2620 [23:28<17:27,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.23988397>, 1399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1500/2620 [25:08<18:40,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.23330711>, 1499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 1600/2620 [26:44<15:38,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.23669158>, 1599)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 1700/2620 [28:22<14:46,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.2536007>, 1699)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 1800/2620 [29:59<13:23,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.26176316>, 1799)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1900/2620 [31:37<11:43,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.24040756>, 1899)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 2000/2620 [33:20<14:18,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.24520172>, 1999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2100/2620 [35:04<08:07,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.23999842>, 2099)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 2200/2620 [36:43<07:07,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.23946735>, 2199)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 2300/2620 [38:27<05:29,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.24607097>, 2299)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 2400/2620 [40:08<03:30,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.24328633>, 2399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 2500/2620 [41:47<01:58,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.24184862>, 2499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 2600/2620 [43:33<00:19,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Autoencoder Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.2423632>, 2599)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2620/2620 [43:54<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "i = 0\n",
    "for step in tqdm(range(train_steps)):\n",
    "    X_ = c.X_train[i]\n",
    "    X_ = X_.reshape(1, X_.shape[0], X_.shape[1])\n",
    "    i +=1\n",
    "    step_e_loss_t0 = train_autoencoder_init(X_)\n",
    "    if i % 100 == 0:\n",
    "        print(('Loss Autoencoder Init', step_e_loss_t0, step))\n",
    "    #with writer.as_default():\n",
    "    #    tf.summary.scalar('Loss Autoencoder Init', step_e_loss_t0, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) RealData with unsupported characters which will be renamed to realdata in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_56_layer_call_fn, gru_cell_56_layer_call_and_return_conditional_losses, gru_cell_57_layer_call_fn, gru_cell_57_layer_call_and_return_conditional_losses, gru_cell_58_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: autoencoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: autoencoder/assets\n"
     ]
    }
   ],
   "source": [
    "autoencoder.save('autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "supervisor_optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_supervisor(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        g_loss_s = mse(h[:, 1:, :], h_hat_supervised[:, :-1, :])\n",
    "\n",
    "    var_list = supervisor.trainable_variables\n",
    "    gradients = tape.gradient(g_loss_s, var_list)\n",
    "    supervisor_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return g_loss_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 262/2620 [03:04<24:26,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Generator Supervised Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.003398405>, 261)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 524/2620 [06:06<26:32,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Generator Supervised Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.0023030809>, 523)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 786/2620 [09:05<21:39,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Generator Supervised Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.0019914063>, 785)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1048/2620 [12:09<16:12,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Generator Supervised Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.0013670538>, 1047)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1310/2620 [15:11<13:16,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Generator Supervised Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.0016143485>, 1309)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1572/2620 [18:16<12:53,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Generator Supervised Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.0006609976>, 1571)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1834/2620 [21:18<08:59,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Generator Supervised Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.0014685576>, 1833)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2096/2620 [24:17<06:21,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Generator Supervised Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.0011175827>, 2095)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 2358/2620 [27:22<02:48,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Generator Supervised Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.0008081817>, 2357)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2620/2620 [30:34<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss Generator Supervised Init', <tf.Tensor: shape=(), dtype=float32, numpy=0.00044688376>, 2619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for step in tqdm(range(train_steps)):\n",
    "    X_ = c.X_train[i]\n",
    "    X_ = X_.reshape(1, X_.shape[0], X_.shape[1])\n",
    "    i +=1\n",
    "    step_g_loss_s = train_supervisor(X_)\n",
    "    if i % int(train_steps/10) == 0:\n",
    "        print(('Loss Generator Supervised Init', step_g_loss_s, step))\n",
    "    #with writer.as_default():\n",
    "    #    tf.summary.scalar('Loss Generator Supervised Init', step_g_loss_s, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) GRU_1_input with unsupported characters which will be renamed to gru_1_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_68_layer_call_fn, gru_cell_68_layer_call_and_return_conditional_losses, gru_cell_69_layer_call_fn, gru_cell_69_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: supervisor/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: supervisor/assets\n"
     ]
    }
   ],
   "source": [
    "supervisor.save(\"supervisor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AdversarialNetSupervised\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " RandomData (InputLayer)     [(None, 250, 25)]         0         \n",
      "                                                                 \n",
      " Generator (Sequential)      (None, 250, 10)           2540      \n",
      "                                                                 \n",
      " Supervisor (Sequential)     (None, 250, 10)           1430      \n",
      "                                                                 \n",
      " Discriminator (Sequential)  (None, 250, 1)            1991      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,961\n",
      "Trainable params: 5,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "E_hat = generator(Z)\n",
    "H_hat = supervisor(E_hat)\n",
    "Y_fake = discriminator(H_hat)\n",
    "\n",
    "adversarial_supervised = Model(inputs=Z,\n",
    "                               outputs=Y_fake,\n",
    "                               name='AdversarialNetSupervised')\n",
    "adversarial_supervised.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AdversarialNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " RandomData (InputLayer)     [(None, 250, 25)]         0         \n",
      "                                                                 \n",
      " Generator (Sequential)      (None, 250, 10)           2540      \n",
      "                                                                 \n",
      " Discriminator (Sequential)  (None, 250, 1)            1991      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,531\n",
      "Trainable params: 4,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Y_fake_e = discriminator(E_hat)\n",
    "\n",
    "adversarial_emb = Model(inputs=Z,\n",
    "                    outputs=Y_fake_e,\n",
    "                    name='AdversarialNet')\n",
    "adversarial_emb.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SyntheticData\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " RandomData (InputLayer)     [(None, 250, 25)]         0         \n",
      "                                                                 \n",
      " Generator (Sequential)      (None, 250, 10)           2540      \n",
      "                                                                 \n",
      " Supervisor (Sequential)     (None, 250, 10)           1430      \n",
      "                                                                 \n",
      " Recovery (Sequential)       (None, 250, 25)           2255      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,225\n",
      "Trainable params: 6,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_hat = recovery(H_hat)\n",
    "synthetic_data = Model(inputs=Z,\n",
    "                       outputs=X_hat,\n",
    "                       name='SyntheticData')\n",
    "synthetic_data.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "def get_generator_moment_loss(y_true, y_pred):\n",
    "    y_true_mean, y_true_var = tf.nn.moments(x=y_true, axes=[0])\n",
    "    y_pred_mean, y_pred_var = tf.nn.moments(x=y_pred, axes=[0])\n",
    "    g_loss_mean = tf.reduce_mean(tf.abs(y_true_mean - y_pred_mean))\n",
    "    g_loss_var = tf.reduce_mean(tf.abs(tf.sqrt(y_true_var + 1e-6) - tf.sqrt(y_pred_var + 1e-6)))\n",
    "    return g_loss_mean + g_loss_var"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DiscriminatorReal\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " RealData (InputLayer)       [(None, 250, 25)]         0         \n",
      "                                                                 \n",
      " Embedder (Sequential)       (None, 250, 10)           2540      \n",
      "                                                                 \n",
      " Discriminator (Sequential)  (None, 250, 1)            1991      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,531\n",
      "Trainable params: 4,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Y_real = discriminator(H)\n",
    "discriminator_model = Model(inputs=X,\n",
    "                            outputs=Y_real,\n",
    "                            name='DiscriminatorReal')\n",
    "discriminator_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "generator_optimizer = Adam()\n",
    "discriminator_optimizer = Adam()\n",
    "embedding_optimizer = Adam()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_generator(x, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_fake = adversarial_supervised(z)\n",
    "        generator_loss_unsupervised = bce(y_true=tf.ones_like(y_fake),\n",
    "                                          y_pred=y_fake)\n",
    "\n",
    "        y_fake_e = adversarial_emb(z)\n",
    "        generator_loss_unsupervised_e = bce(y_true=tf.ones_like(y_fake_e),\n",
    "                                            y_pred=y_fake_e)\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "\n",
    "        x_hat = synthetic_data(z)\n",
    "        generator_moment_loss = get_generator_moment_loss(x, x_hat)\n",
    "\n",
    "        generator_loss = (generator_loss_unsupervised +\n",
    "                          generator_loss_unsupervised_e +\n",
    "                          100 * tf.sqrt(generator_loss_supervised) +\n",
    "                          100 * generator_moment_loss)\n",
    "\n",
    "    var_list = generator.trainable_variables + supervisor.trainable_variables\n",
    "    gradients = tape.gradient(generator_loss, var_list)\n",
    "    generator_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return generator_loss_unsupervised, generator_loss_supervised, generator_moment_loss\n",
    "\n",
    "@tf.function\n",
    "def train_embedder(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "\n",
    "        x_tilde = autoencoder(x)\n",
    "        embedding_loss_t0 = mse(x, x_tilde)\n",
    "        e_loss = 10 * tf.sqrt(embedding_loss_t0) + 0.1 * generator_loss_supervised\n",
    "\n",
    "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "    gradients = tape.gradient(e_loss, var_list)\n",
    "    embedding_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return tf.sqrt(embedding_loss_t0)\n",
    "\n",
    "@tf.function\n",
    "def get_discriminator_loss(x, z):\n",
    "    y_real = discriminator_model(x)\n",
    "    discriminator_loss_real = bce(y_true=tf.ones_like(y_real),\n",
    "                                  y_pred=y_real)\n",
    "\n",
    "    y_fake = adversarial_supervised(z)\n",
    "    discriminator_loss_fake = bce(y_true=tf.zeros_like(y_fake),\n",
    "                                  y_pred=y_fake)\n",
    "\n",
    "    y_fake_e = adversarial_emb(z)\n",
    "    discriminator_loss_fake_e = bce(y_true=tf.zeros_like(y_fake_e),\n",
    "                                    y_pred=y_fake_e)\n",
    "    return (discriminator_loss_real +\n",
    "            discriminator_loss_fake +\n",
    "            gamma * discriminator_loss_fake_e)\n",
    "\n",
    "@tf.function\n",
    "def train_discriminator(x, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        discriminator_loss = get_discriminator_loss(x, z)\n",
    "\n",
    "    var_list = discriminator.trainable_variables\n",
    "    gradients = tape.gradient(discriminator_loss, var_list)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return discriminator_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "idata = iter(c.X_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.98814549,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.98814549,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.98799528,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [-0.98548323,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.98548323,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.98548323,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ]])"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(idata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_540553/3486469369.py\", line 4, in train_generator  *\n        y_fake = adversarial_supervised(z)\n    File \"/home/jmeow/miniconda3/envs/meow2/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/jmeow/miniconda3/envs/meow2/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"AdversarialNetSupervised\" is incompatible with the layer: expected shape=(None, 250, 25), found shape=(250, 25)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [173]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m Z_ \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m250\u001B[39m,\u001B[38;5;241m25\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Train generator\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m step_g_loss_u, step_g_loss_s, step_g_loss_v \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mZ_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Train embedder\u001B[39;00m\n\u001B[1;32m     14\u001B[0m step_e_loss_t0 \u001B[38;5;241m=\u001B[39m train_embedder(X_)\n",
      "File \u001B[0;32m~/miniconda3/envs/meow2/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/tmp/__autograph_generated_filepo5gt_l5.py:11\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_generator\u001B[0;34m(x, z)\u001B[0m\n\u001B[1;32m      9\u001B[0m retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefinedReturnValue()\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[0;32m---> 11\u001B[0m     y_fake \u001B[38;5;241m=\u001B[39m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43madversarial_supervised\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     generator_loss_unsupervised \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(bce), (), \u001B[38;5;28mdict\u001B[39m(y_true\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mones_like, (ag__\u001B[38;5;241m.\u001B[39mld(y_fake),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope), y_pred\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(y_fake)), fscope)\n\u001B[1;32m     13\u001B[0m     y_fake_e \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(adversarial_emb), (ag__\u001B[38;5;241m.\u001B[39mld(z),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n",
      "File \u001B[0;32m~/miniconda3/envs/meow2/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/miniconda3/envs/meow2/lib/python3.10/site-packages/keras/engine/input_spec.py:264\u001B[0m, in \u001B[0;36massert_input_compatibility\u001B[0;34m(input_spec, inputs, layer_name)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec_dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m spec_dim \u001B[38;5;241m!=\u001B[39m dim:\n\u001B[0;32m--> 264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    265\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mincompatible with the layer: \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    266\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexpected shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspec\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    267\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfound shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdisplay_shape(x\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/tmp/ipykernel_540553/3486469369.py\", line 4, in train_generator  *\n        y_fake = adversarial_supervised(z)\n    File \"/home/jmeow/miniconda3/envs/meow2/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/jmeow/miniconda3/envs/meow2/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"AdversarialNetSupervised\" is incompatible with the layer: expected shape=(None, 250, 25), found shape=(250, 25)\n"
     ]
    }
   ],
   "source": [
    "idata = iter(c.X_train)\n",
    "step_g_loss_u = step_g_loss_s = step_g_loss_v = step_e_loss_t0 = step_d_loss = 0\n",
    "for step in range(train_steps):\n",
    "    # Train generator (twice as often as discriminator)\n",
    "    j = 0\n",
    "    for kk in range(2):\n",
    "        X_ = next(idata)\n",
    "        X_ = X_.reshape(1, X_.shape[0], X_.shape[1])\n",
    "        Z_ = np.random.rand(250,25)\n",
    "\n",
    "        # Train generator\n",
    "        step_g_loss_u, step_g_loss_s, step_g_loss_v = train_generator(X_, Z_)\n",
    "        # Train embedder\n",
    "        step_e_loss_t0 = train_embedder(X_)\n",
    "\n",
    "    X_ = next(idata)\n",
    "    X_ = X_.reshape(1, X_.shape[0], X_.shape[1])\n",
    "    Z_ = np.random.rand(250,25)\n",
    "    step_d_loss = get_discriminator_loss(X_, Z_)\n",
    "    if step_d_loss > 0.15:\n",
    "        step_d_loss = train_discriminator(X_, Z_)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f'{step:6,.0f} | d_loss: {step_d_loss:6.4f} | g_loss_u: {step_g_loss_u:6.4f} | '\n",
    "              f'g_loss_s: {step_g_loss_s:6.4f} | g_loss_v: {step_g_loss_v:6.4f} | e_loss_t0: {step_e_loss_t0:6.4f}')\n",
    "\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('G Loss S', step_g_loss_s, step=step)\n",
    "        tf.summary.scalar('G Loss U', step_g_loss_u, step=step)\n",
    "        tf.summary.scalar('G Loss V', step_g_loss_v, step=step)\n",
    "        tf.summary.scalar('E Loss T0', step_e_loss_t0, step=step)\n",
    "        tf.summary.scalar('D Loss', step_d_loss, step=step)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meow2",
   "language": "python",
   "name": "meow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
